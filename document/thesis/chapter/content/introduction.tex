\chapter{Introduction} Huge influence of computers in society has made smart devices, an important part of our lives. Availability, affordability and functionality of such devices motivated us to use them in our day-to-day living. The list of smart devices includes personal automatic and semi-automatic robots which are also playing a major role in our household. For an instance, Roomba is an autonomous robotic vacuum cleaners that automatically cleans the floor and goes to its charging station without human interaction \cite{4}.

Interaction with smart devices is still being mostly through displays, keyboards, mouse and touch interfaces. These devices have grown to be familiar but inherently limit the speed and naturalness with which we can interact with the computer. Usage of robots for domestic and industrial purposes has been continuously increasing \cite{5}. Thus in recent years, there has been a tremendous push in research toward an intuitive and easy communication with the robot through speech, gestures and facial expressions.

Tremendous progress have been made in speech recognition and several commercially successful speech interfaces are available \cite{6}. However, speech recognition systems have certain limitations such as misinterpretation due to various accents and background noise interference.

Furthermore, there has been an increased interest in recent years in trying to introduce other human-to-human communication modalities into HRI. This includes a class of techniques based on the movement of the human arm or hand. The use of hand gestures provides an attractive alternative for Human-robot interaction than the conventional cumbersome devices.


\section{Goal} As described earlier, HRI research is focusing to build an intuitive and easy communication with the robot through speech, gestures and facial expressions. The use of hand gestures provides the ease and naturalness with which the user can interact with robots \cite{1}. Our goal in this thesis to implement a system that should be integrated into NAO to recognize hand gestures. 

Existing cameras of NAO are greatly limited by the quality of the input image. Variations in lighting and background clutters would only worsen the problem \cite{17}. On the other hand, depth-based approaches are able to provide satisfactory results for hand gesture recognition even with poor indoor lighting and cluttered background condition \cite{18}. Therefore, we have chosen Asus Xtion which has sensors that capture both RGB and depth data. Asus Xtion is an OpenNI compatible device, thus, we have chosen a NiTE middleware for the purpose of tracking the human skeletal points. 

We have chosen Gesture Recognition Toolkit (GRT) \cite{16} to train and predict the 3D skeletal modeled gestures with feature based statistical learning algorithm. Adaptive Naive Bayes Classifier (ANBC) is the supervised machine learning algorithm which is chosen for the purpose of classifying and predicting the hand gestures in real time. Furthermore, all these interactions must be displayed to visually understand the status of the system. Finally, recognized hand gestures must be translated to robotic actions as following :
\begin{itemize}
	\item \textbf{Gesture-to-Speech} should translate the recognized gestures and it should be spoken out loud using the integrated loudspeaker.
	
	\item \textbf{Gesture-to-Motion} should move the robot from one position to another in the 2 dimensional space. Therefore, each gesture should be assigned to a locomotion task.
	
	\item \textbf{Gesture-to-Gesture} should translate the human hand gesture to a robotic hand gesture by imitating hand gestures of the user. 
\end{itemize}

The goal should be reached by studying the various solution to this problem and an appropriate design must be chosen. The main challenge is to find a solution that can integrate all these components into a robust system. Furthermore, this system must be tested and results must be presented clearly. Evaluations must be carried out to demonstrate the effectiveness of the classifier and to validate its potential for real time gesture recognition.

\section{Outline}

\paragraph*{Background} To begin with, we have studied key concepts, state of the art solutions and tools available to accomplish this goal. Chapter \ref{ch:background} thoroughly discusses about the humanoid robot and stages of hand gesture recognition with the help of computer vision and machine learning algorithms. In details, this chapter explains the concepts of gesture modeling, feature extraction from depth image by NiTE framework, learning and classification of hand gestures using Adaptive Naive Bayes Classifier with the help of GRT.

\paragraph*{Hand Gesture Recognition for Human-Robot Interaction} In addition to that, Chapter \ref{ch:solution} talks about the functional blocks of system and how they are implemented to provide an efficient solution. Furthermore, it illustrates the functionalities of Human-Robot Interaction (HRI) module, Brain module, Control Center (CC) module and Command module, and how they all are integrated into a robust system.

\paragraph*{Results and Evaluation} Finally, Chapter \ref{ch:result} shows promising results in recognizing five static hand gestures which are trained in lab condition. It shows not only the performances of gesture prediction, but also the human-robot interactions where the robot has executed predefined tasks, when the hand gestures are recognized. Evaluation demonstrates the effectiveness of the system and its potential for real time recognition application. Finally, metrics such as mean, standard deviation, confusion matrix, precision, recall, F-Measure are presented.

\paragraph*{Conclusion} In a conclusion, this manuscript demonstrates how the goal of building a hand gesture recognition based on skeletal points tracking using depth camera is accomplished, as well as, how this system can be improved in several ways by proposing various alternatives.
