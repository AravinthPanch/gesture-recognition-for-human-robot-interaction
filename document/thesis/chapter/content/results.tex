\chapter{Results} \label{ch:result} During the training sessions, we have recorded five static gestures in 4 different positions in front of the robot. NAO is equipped with Asus Xtion and set in "Stand" posture. Head pitch of NAO is set to -18 degrees to look at the upper body of the user 1800 mm away from the sensor. First 3 positions of training are recorded at 1800 mm distance from the sensor in z axis and + /- 800 mm in x axis. Last training position is recorded at 2200 mm distance. Therefore, training data is recorded for 80 seconds of each gesture.

In this chapter, we present the results of real time hand gesture recognition for Human-robot interaction based on skeletal points tracking using depth camera. Training data for 5 classes with 11918 samples of 6 dimensional vector are trained with Adaptive Naive Bayes Classifier (ANBC). Min-Max scaling and Null Rejection with coefficient of 2.0 are enabled. Following sections illustrates the results of end-to-end interaction with robot using five gestures named as Walk, Turn Right, Turn Left, Move Right, Move Left gestures which are mapped to class labels 1,2,3,4,5 respectively.

\clearpage 
\section{Gesture-To-Motion} 
\subsection{Walk} Figure \ref{res:gm:walk} shows that NAO is looking at the user to detect any possible gestures. When it recognized Walk gesture, Command module commands the robot to walk in forward direction for five seconds, thereby, completing a Gesture-To-Motion translation.

\input{chapter/figures/res-walk}

Figure \ref{res:pl:walk} shows the positions of left and right hand in x and y coordinates from the origin of the sensor. It is plotted using 60 input samples of Walk gesture. Figure \ref{res:cc:walk} shows the dashboard of Control Center that displays the prediction results. 

\clearpage 
\subsection{Turn Right} Figure \ref{res:gm:turn:right} shows that NAO received the input samples of left and right hand, and it recognized Turn Right gesture. The user commands NAO to turn at his right direction, thus, NAO turns at the mirrored direction (Left) for 5 seconds and stops to look for further gestures.

\input{chapter/figures/res-turn-right}

Figure \ref{res:pl:turn:right} shows the positions of left and right hand in x and y coordinates from the origin of the sensor. It is plotted using 60 input samples of Turn Right gesture. Figure \ref{res:cc:turn:right} shows the dashboard of Control Center that displays the prediction results. 

\clearpage 
\subsection{Turn Left} Figure \ref{res:gm:turn:left} shows that NAO received the input samples of left and right hand, and it recognized Turn Left gesture. The user commands NAO to turn at his left direction, thus, NAO turns at the mirrored direction (Right) for 5 seconds and stops to look for further gestures.

\input{chapter/figures/res-turn-left}

Figure \ref{res:pl:turn:left} shows the positions of left and right hand in x and y coordinates from the origin of the sensor. It is plotted using 60 input samples of Turn Left gesture. Figure \ref{res:cc:turn:left} shows the dashboard of Control Center that displays the prediction results. 

\clearpage 
\subsection{Move Right} Figure \ref{res:gm:move:right} shows that NAO recognizes Move Right gesture as soon as the user gesticulated it using both the tracked hands. Like other gestures, this is also perceived by NAO to move in the mirrored direction of the user. Hence, NAO turns left for 3 seconds and move in forward direction for 5 seconds.

\input{chapter/figures/res-move-right}

Figure \ref{res:pl:move:right} shows the positions of left and right hand in x and y coordinates from the origin of the sensor. It is plotted using 60 input samples of Move Right gesture. Figure \ref{res:cc:move:right} shows the dashboard of Control Center that displays the prediction results. 

\clearpage 
\subsection{Move Left} Figure \ref{res:gm:move:left} shows that NAO recognizes Move Left gesture as soon as the user gesticulated it using both the tracked hands. Like other gestures, this is also perceived by NAO to move in the mirrored direction of the user. Hence, NAO turns right for 3 seconds and move in forward direction for 5 seconds.

\input{chapter/figures/res-move-left}

Figure \ref{res:pl:move:left} shows the positions of left and right hand in x and y coordinates from the origin of the sensor. It is plotted using 60 input samples of Move Left gesture. Figure \ref{res:cc:move:left} shows the dashboard of Control Center that displays the prediction results. 

\clearpage

\section{Gesture-To-Gesture} Figure \ref{res:gg} shows how human hand gestures are translated to robotic hand gestures. When a gesture is detected, the Command module sets the predefined angles to the shoulder and elbow joints of both the hands of NAO to perform Gesture-To-Gesture translation. 

\input{chapter/figures/res-gg}

\section{Evaluation} We have used a machine learning toolkit named as GRT to recognize hand gestures using skeletal points tracking algorithm and Adaptive Naive Bayes classifier (ANBC) for classification and prediction. 

The classifier is based on a statistical model of x,y,z coordinate positions of static hand gestures and provides a likelihood measure for recognized gesture. Furthermore, the gesture recognition pipeline uses two post processing modules such as Class Label Filter and Class Label Change Filter to exclude lower frequent spikes in the prediction results and trigger an output only when there is a change in prediction.

In this section, we present the experiments carried out to evaluate and validate our system to recognize hand gestures using skeletal points. The goal is to demonstrate the effectiveness of the classifier and to evaluate its potential for real time input at 30 fps. In the classification phase, input samples are normalized using Min-Max Scaling and Null Rejection is enabled to detect non-gestures. Therefore, the evaluation consists of computing the prediction accuracy for various null rejection coefficient and compare it with other supervised learning classifiers such as Support Vector Machine (SVM) and Minimum Distance (MinDist).

\subsection{Mean and Standard Deviation} ANBC is a supervised learning algorithm that can be used to classify any type of N-dimensional signal. It fundamentally works by fitting an N-dimensional Gaussian distribution to each class when it is trained.

During the training phase, first all the input samples are normalized using Min-Max Scaling with the range from 0 to 1 and then GRT computes mean $\mu$ and standard deviation $\sigma$ to create a model for each class. During the prediction phase, it basically computes the maximum a posterior probability of an input vector belonging to any of the trained class. Figure \ref{fg:ev:mean} shows the mean positions of left and right hand for every gesture. Table \ref{tb:ev:mean} and \ref{tb:ev:sd} show mean and standard deviations of the labeled training data of all the five classes. 

\subsection{Classification and Prediction} Our gesture recognition pipeline is trained with 11918 input samples of 6 dimensional vector for 5 classes. Class 1,2,3,4,5 are mapped to Walk, Turn Right, Turn Left, Move Right, Move Left gestures respectively. We have carried out experiments to evaluate the classification, prediction and post processing efficiency of our system. Figure \ref{ev:test:prediction} show change is positions of left and hand in Cartesian coordinates while test data is recorded and corresponding prediction for every input sample.

Test data consists of 1400 samples which are recorded under supervised arrangement. Input vectors that is not containing left and right hand are removed from the test data. Furthermore, input vectors which are recorded when the hand is at the field of view of the camera are also excluded. A program is implemented with GRT to read all the test data and execute prediction on every input sample and then results are stored to CSV file. Finally results are plotted using MATLAB.

Prediction results shown in the graph \ref{ev:test:prediction} frequently falls down to Class Label 0 that is reserved for non-gestures. Non-gestures are detected with the help of Null Rejection thresholds. Therefore, this prediction is based on normalized classification data for Adaptive Naive Bayes classifier with Null rejection coefficient of 1.0.

\subsection{Prediction Accuracy Vs Null Rejection Accuracy} \label{sec:ev:accuracy} Classifiers of GRT offers various customization that could produce different results for the same test data. Accuracy of a gesture recognition system does not depend only on th precise predictions of trained gestures, but also differentiating them from unintended hand gestures. Figure \ref{ev:accuracy:anbc} shows the prediction and null rejection accuracies of Adaptive Naive Bayes Classifier (ANBC). Figure shows the accuracies of 5 trained gestures and a non-gesture. Non-Gesture data is recorded with both the hands are pointing downwards. GRT allows us to set null rejection coefficient for the classifier and therefore, graph contains the dataset of accuracies with null rejection coefficients range from 0 to 10.

Figure \ref{ev:accuracy:anbc} shows that increase in null rejection coefficient causes an increase in the accuracy of trained gestures, however, causes a decrease in the accuracy of non-gesture. Therefore, it is optimal to use a null rejection coefficient of 2.0 with ANBC.

Figure \ref{ev:accuracy:svm} shows prediction results of Support Vector Machine classifier with RBF Kernel. It is apparent that accuracy of trained gestures are consistently above 95 percent. However, null rejection accuracy is constantly lesser than 10 percent, thus, denoting that SVM is not useful in our case, because the robot should not execute any unintended commands.

Figure \ref{ev:accuracy:mindist} shows some interesting results of Minimum Distance classifier with 4 clusters. Figure shows that prediction results are unpredictable as there is increase in null rejection coefficient. However, with coefficient of around 6.3, MinDist shows compromising accuracy above 90 percent for trained gesture and non-gesture. This shows that MinDist is a better alternative to ANBC. 

\input{chapter/tables/ev-mean-sd}

\clearpage 
\input{chapter/figures/ev-all-ges-mean} 
\input{chapter/figures/ev-test-all} 
\input{chapter/figures/ev-accuracy}

