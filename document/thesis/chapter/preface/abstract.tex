\addcontentsline{toc}{chapter}{\numberline{}Abstract}
\chapter*{Abstract}
Human-robot interaction (HRI) has been a topic of both science fiction and academic speculation even before any robots existed \cite{7}. HRI research is focusing to build an intuitive and easy communication with the robot through speech, gestures, and facial expressions. The use of hand gestures provides an attractive alternative to complex interfaced devices for HRI. In particular, visual interpretation of hand gestures can help in achieving the ease and naturalness desired for HRI. This has motivated a very active research concerned with computer vision-based analysis and interpretation of hand gestures.

In this thesis, we aim to implement the hand gesture recognition for robots with modeling, training, analyzing and recognizing gestures based on computer vision and machine learning techniques. Gestures are modeled based on skeletal points and the features are extracted using NiTE framework using a depth camera. 

In order to recognize gestures, we propose to learn and classify gestures with the help of Adaptive Naive Bayes Classifier (ANBC) using Gesture Recognition Toolkit (GRT). Furthermore, we aim to build a dashboard that can visualize the interaction between all the components. Finally, we attempt to integrate all these functionalities into a system that could operate with a humanoid robot NAO.

As a result, on the one side, gestures will be used command the robot to execute certain actions and on the other side, gestures will be translated and spoken out by the robot. 


\subsection*{Keywords} Human-Robot Interaction (HRI), Hand Gesture Recognition, Humanoid Robot, NAO, Computer Vision, Skeletal Points Tracking, NiTE, Depth Camera, Asus Xtion Pro, Machine Learning, Adaptive Naive Bayes Classifier (ANBC), Gesture Recognition Toolkit (GRT)


